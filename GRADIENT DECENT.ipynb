{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23f094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c122d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x,y):\n",
    "    weight = 0.1\n",
    "    bias = 0.1\n",
    "    learning_rate = 0.05\n",
    "    n = len(x)\n",
    "    \n",
    "    iteration = 100\n",
    "    \n",
    "    for i in range(iteration):\n",
    "        #forward propagation\n",
    "        ypred = weight*x + bias\n",
    "        mse = mean_squared_error(y,ypred)\n",
    "        \n",
    "        \n",
    "        #back propagation\n",
    "        derivation_weight = -(2/n)*sum(x*(y-ypred))\n",
    "        d_bias = -(2/n)*sum(y-ypred)\n",
    "        \n",
    "        #update the weight and bias\n",
    "        weight = weight - (learning_rate*derivation_weight)\n",
    "        bias = bias -(learning_rate*d_bias)\n",
    "        \n",
    "        \n",
    "        print(f\"Iteration: {i} weight: {weight} bias: {bias} MSE {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a17e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,5,6,3,4])\n",
    "y = np.array([10,12,13,15,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d077eee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 weight: 4.868 bias: 1.3320000000000003 MSE 154.54000000000002\n",
      "Iteration: 1 weight: 0.8715199999999999 bias: 0.6289600000000001 MSE 111.23713920000004\n",
      "Iteration: 2 weight: 4.0960704 bias: 1.5148864000000004 MSE 81.67289166847999\n",
      "Iteration: 3 weight: 1.3732510719999986 bias: 1.086891008 MSE 61.43738868572164\n",
      "Iteration: 4 weight: 3.550775623680001 bias: 1.7363664998400006 MSE 47.53792174301853\n",
      "Iteration: 5 weight: 1.6926067685375992 bias: 1.4934351128576 MSE 37.94346871915317\n",
      "Iteration: 6 weight: 3.159965648396289 bias: 1.9809010295275524 MSE 31.27561060026237\n",
      "Iteration: 7 weight: 1.8888830289662764 bias: 1.8620239801842073 MSE 26.598788362950557\n",
      "Iteration: 8 weight: 2.8746574460949565 bias: 2.2380460311586017 MSE 23.277971782163\n",
      "Iteration: 9 weight: 2.0022959980494637 bias: 2.201871598526658 MSE 20.882073762555972\n",
      "Iteration: 10 weight: 2.661589754003267 bias: 2.5008119594151963 MSE 19.118407442204678\n",
      "Iteration: 11 weight: 2.0601150374598074 bias: 2.519326656952435 MSE 17.788208828302878\n",
      "Iteration: 12 weight: 2.498170742637817 bias: 2.764550277022465 MSE 16.756454441119622\n",
      "Iteration: 13 weight: 2.0808245451794787 bias: 2.818790367117848 MSE 15.931379510175677\n",
      "Iteration: 14 weight: 2.3690494970624036 bias: 3.0261980032378615 MSE 15.250578939224095\n",
      "Iteration: 15 weight: 2.0769481309434337 bias: 3.1033393940303617 MSE 14.67157532196897\n",
      "Iteration: 16 weight: 2.2637894133703216 bias: 3.2837651648688206 MSE 14.165418288819534\n",
      "Iteration: 17 weight: 2.0569650714558105 bias: 3.3751486713012167 MSE 13.712340909707962\n",
      "Iteration: 18 weight: 2.175289352028238 bias: 3.535987077017887 MSE 13.2988120129306\n",
      "Iteration: 19 weight: 2.026610790232307 bias: 3.635778415545368 MSE 12.915535772345516\n",
      "Iteration: 20 weight: 2.098712217320853 bias: 3.7820884737025544 MSE 12.556094111911587\n",
      "Iteration: 21 weight: 1.9897593391755986 bias: 3.886368983750375 MSE 12.216025327949133\n",
      "Iteration: 22 weight: 2.0307578751849147 bias: 4.02162353648861 MSE 11.892198731150696\n",
      "Iteration: 23 weight: 1.9490222284974914 bias: 4.127773190269481 MSE 11.582390170322508\n",
      "Iteration: 24 weight: 1.9691697386094535 bias: 4.254367424413487 MSE 11.284993877381428\n",
      "Iteration: 25 weight: 1.9061547721518795 bias: 4.360646181300546 MSE 10.99882682295939\n",
      "Iteration: 26 weight: 1.9123999197134014 bias: 4.480242749752778 MSE 10.72299585273882\n",
      "Iteration: 27 weight: 1.8623318145060272 bias: 4.585506505286407 MSE 10.456807429811962\n",
      "Iteration: 28 weight: 1.859381985256705 bias: 4.699269765245476 MSE 10.199706292468571\n",
      "Iteration: 29 weight: 1.8183348201167577 bias: 4.80277763432338 MSE 9.951233736916915\n",
      "Iteration: 30 weight: 1.8093767320707148 bias: 4.9115326392466745 MSE 9.710999220340856\n",
      "Iteration: 31 weight: 1.7746788153539346 bias: 5.012816217135136 MSE 9.478661005919076\n",
      "Iteration: 32 weight: 1.7618675141267368 bias: 5.1171566455871265 MSE 9.253912946424876\n",
      "Iteration: 33 weight: 1.7316985142231065 bias: 5.215931325660254 MSE 9.036475436092813\n",
      "Iteration: 34 weight: 1.7164891957240047 bias: 5.316292757689448 MSE 8.826089193618492\n",
      "Iteration: 35 weight: 1.6896067472422465 bias: 5.412397587545382 MSE 8.622510968834332\n",
      "Iteration: 36 weight: 1.6729799237734926 bias: 5.509107264838789 MSE 8.425510557184015\n",
      "Iteration: 37 weight: 1.6485340957688757 bias: 5.602464167320983 MSE 8.234868703986\n",
      "Iteration: 38 weight: 1.6311483855490583 bias: 5.695774794196712 MSE 8.050375614751372\n",
      "Iteration: 39 weight: 1.6085557728989464 bias: 5.786360928268398 MSE 7.87182987894227\n",
      "Iteration: 40 weight: 1.5908515753127885 bias: 5.876473641739959 MSE 7.699037676393717\n",
      "Iteration: 41 weight: 1.5697098504073521 bias: 5.964302678947103 MSE 7.5318121775862625\n",
      "Iteration: 42 weight: 1.5519796926986602 bias: 6.051382667897599 MSE 7.369973077435773\n",
      "Iteration: 43 weight: 1.532009613601904 bias: 6.136492117882348 MSE 7.213346221594884\n",
      "Iteration: 44 weight: 1.514445881139299 bias: 6.22067925292539 MSE 7.061763297378336\n",
      "Iteration: 45 weight: 1.495451931845271 bias: 6.3031218927999175 MSE 6.9150615703283025\n",
      "Iteration: 46 weight: 1.4781792511705307 bias: 6.384537969418723 MSE 6.773083653479761\n",
      "Iteration: 47 weight: 1.4600229257546926 bias: 6.464376057032049 MSE 6.635677300489253\n",
      "Iteration: 48 weight: 1.4431201332693488 bias: 6.543129739542061 MSE 6.502695216576595\n",
      "Iteration: 49 weight: 1.4257018003546986 bias: 6.620431114945502 MSE 6.373994883121625\n",
      "Iteration: 50 weight: 1.4092168440582322 bias: 6.696621319316166 MSE 6.2494383930439605\n",
      "Iteration: 51 weight: 1.392463434056765 bias: 6.771456786642421 MSE 6.128892294968037\n",
      "Iteration: 52 weight: 1.376423479873874 bias: 6.8451750030366085 MSE 6.012227444770397\n",
      "Iteration: 53 weight: 1.360280123739422 bias: 6.917616580380876 MSE 5.899318863511546\n",
      "Iteration: 54 weight: 1.344698407888095 bias: 6.988948475321807 MSE 5.790045601030948\n",
      "Iteration: 55 weight: 1.329122757540523 bias: 7.059068232792151 MSE 5.684290604672855\n",
      "Iteration: 56 weight: 1.3140032309589955 bias: 7.128094761647537 MSE 5.58194059274022\n",
      "Iteration: 57 weight: 1.2989615996642794 bias: 7.195964057718365 MSE 5.482885932363312\n",
      "Iteration: 58 weight: 1.2843020743154545 bias: 7.262762244074103 MSE 5.387020521531509\n",
      "Iteration: 59 weight: 1.2697668122584047 bias: 7.328451231426819 MSE 5.294241675079984\n",
      "Iteration: 60 weight: 1.255561090986589 bias: 7.393094719625943 MSE 5.204450014453629\n",
      "Iteration: 61 weight: 1.241508799212066 bias: 7.456672033088445 MSE 5.11754936109236\n",
      "Iteration: 62 weight: 1.227748116009462 bias: 7.519231486079016 MSE 5.033446633298066\n",
      "Iteration: 63 weight: 1.214158429442972 bias: 7.580764053387519 MSE 4.952051746455351\n",
      "Iteration: 64 weight: 1.2008324219249438 bias: 7.641307444860438 MSE 4.8732775164873905\n",
      "Iteration: 65 weight: 1.1876871787285752 bias: 7.700860380042915 MSE 4.797039566435502\n",
      "Iteration: 66 weight: 1.1747845433245467 bias: 7.7594532141217645 MSE 4.723256236056947\n",
      "Iteration: 67 weight: 1.1620672165735648 bias: 7.81708976624626 MSE 4.651848494340444\n",
      "Iteration: 68 weight: 1.149576148561983 bias: 7.87379524732368 MSE 4.582739854843181\n",
      "Iteration: 69 weight: 1.137271456081134 bias: 7.929576786137758 MSE 4.5158562937569595\n",
      "Iteration: 70 weight: 1.1251799437676127 bias: 7.984455954213152 MSE 4.451126170614527\n",
      "Iteration: 71 weight: 1.113273579010969 bias: 8.038441980160144 MSE 4.388480151550456\n",
      "Iteration: 72 weight: 1.1015695990710284 bias: 8.09155382211996 MSE 4.327851135033734\n",
      "Iteration: 73 weight: 1.0900480442818536 bias: 8.143801992260974 MSE 4.26917417999226\n",
      "Iteration: 74 weight: 1.0787196901722582 bias: 8.195203536207773 MSE 4.212386436251938\n",
      "Iteration: 75 weight: 1.0675700855135752 bias: 8.245769700321537 MSE 4.157427077215763\n",
      "Iteration: 76 weight: 1.0566056505977706 bias: 8.295516097794225 MSE 4.104237234710616\n",
      "Iteration: 77 weight: 1.0458157013958442 bias: 8.34445434078765 MSE 4.052759935931929\n",
      "Iteration: 78 weight: 1.0352037314677685 bias: 8.392598940178464 MSE 4.002940042418642\n",
      "Iteration: 79 weight: 1.0247616414460348 bias: 8.439961628202866 MSE 3.9547241909930393\n",
      "Iteration: 80 weight: 1.0144909666128452 bias: 8.486556041633087 MSE 3.908060736602174\n",
      "Iteration: 81 weight: 1.0043853888859218 bias: 8.532393870156897 MSE 3.8628996969996536\n",
      "Iteration: 82 weight: 0.9944451415647971 bias: 8.577488035364556 MSE 3.8191926992085072\n",
      "Iteration: 83 weight: 0.9846651418035185 bias: 8.621850078033479 MSE 3.7768929277078067\n",
      "Iteration: 84 weight: 0.9750447654126744 bias: 8.665492316344794 MSE 3.7359550742875114\n",
      "Iteration: 85 weight: 0.9655797933835991 bias: 8.708426073853499 MSE 3.6963352895178425\n",
      "Iteration: 86 weight: 0.9562690448318072 bias: 8.750663144982381 MSE 3.6579911357811903\n",
      "Iteration: 87 weight: 0.947108911731158 bias: 8.792214593448056 MSE 3.620881541816243\n",
      "Iteration: 88 weight: 0.9380978598086817 bias: 8.83309174764541 MSE 3.584966758725654\n",
      "Iteration: 89 weight: 0.9292327196363198 bias: 8.87330538615357 MSE 3.5502083174001378\n",
      "Iteration: 90 weight: 0.9205117407307668 bias: 8.912866414076412 MSE 3.5165689873133488\n",
      "Iteration: 91 weight: 0.9119320745101962 bias: 8.95178531119108 MSE 3.48401273664346\n",
      "Iteration: 92 weight: 0.9034918466098448 bias: 8.990072591758096 MSE 3.4525046936786885\n",
      "Iteration: 93 weight: 0.8951884486406383 bias: 9.027738430870546 MSE 3.422011109465435\n",
      "Iteration: 94 weight: 0.8870199442751201 bias: 9.064792977300048 MSE 3.3924993216590495\n",
      "Iteration: 95 weight: 0.878983909862393 bias: 9.101246100745497 MSE 3.363937719538478\n",
      "Iteration: 96 weight: 0.8710783884185402 bias: 9.137107604923237 MSE 3.3362957101473207\n",
      "Iteration: 97 weight: 0.8633011026994502 bias: 9.172387056831868 MSE 3.309543685525058\n",
      "Iteration: 98 weight: 0.855650102406297 bias: 9.20709393212289 MSE 3.2836529909933097\n",
      "Iteration: 99 weight: 0.848123230012642 bias: 9.241237499996208 MSE 3.258595894463184\n"
     ]
    }
   ],
   "source": [
    "gradient(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2846f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
